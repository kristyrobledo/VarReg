<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="VarReg">
<title>VarReg: Under the hood • VarReg</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="VarReg: Under the hood">
<meta property="og:description" content="VarReg">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-dark navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">VarReg</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.0.1</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/VarReg.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/under_the_hood.html">VarReg: Under the hood</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/kristyrobledo/VarReg/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">


<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>VarReg: Under the hood</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/kristyrobledo/VarReg/blob/HEAD/vignettes/under_the_hood.Rmd" class="external-link"><code>vignettes/under_the_hood.Rmd</code></a></small>
      <div class="d-none name"><code>under_the_hood.Rmd</code></div>
    </div>

    
    
<p>There are three main algorithms within the package.</p>
<ol style="list-style-type: decimal">
<li>models for the mean and the variance</li>
<li>models for the mean and the variance, taking into account censoring of the outcome variable</li>
<li>models for the location, scale and shape</li>
</ol>
<p>Here we will discuss some of the specifics of each of these algorithms.</p>
<div class="section level2" number="1">
<h2 id="mean-and-variance-models">
<span class="header-section-number">1</span> Mean and Variance models<a class="anchor" aria-label="anchor" href="#mean-and-variance-models"></a>
</h2>
<p>The statistical methods for the mean and variance model given in Robledo and Marschner <span class="citation">(<a href="#ref-Robledo2021">2021</a>)</span>. Briefly, let a set of covariates be included on which the mean depends be <span class="math inline">\(P\)</span>, giving <span class="math inline">\(\boldsymbol{z}_i = (z_{i1}, \dots, z_{iP})^T\)</span> and the number of covariates on which the variance depends be <span class="math inline">\(Q\)</span> giving <span class="math inline">\(\boldsymbol{x}_i = (x_{i1}, \dots, x_{iQ})^T\)</span>. This leads to the general model</p>
<p><span class="math display" id="eq:general">\[\begin{align}
X_i \sim N\left(\boldsymbol{\beta}^T \boldsymbol{z} , \boldsymbol{\alpha}^T \boldsymbol{x} \right) \quad \textrm{for }\quad i=1, 2,...,n  \tag{1.1}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\beta} =( \beta_0,...,\beta_P)^T\)</span> and <span class="math inline">\(\boldsymbol{\alpha} = (\alpha_0, ..., \alpha_Q)^T\)</span>. The mean is estimated using linear regression, weighted by the inverse of the current estimate of the variance. The covariates (<span class="math inline">\(\boldsymbol{z}_i\)</span>) are fit in this model, and updated estimates of <span class="math inline">\(\boldsymbol{\hat\beta}_i\)</span> are obtained, where <span class="math inline">\(\hat{\boldsymbol{\beta}}=(\hat{\beta}_0, .., \hat{\beta}_P)\)</span>. This additional step to fit the mean model in both the E-step and M-step converts the EM algorithm to a ECME (Expectation Conditional Maximization Either) algorithm <span class="citation">(<a href="#ref-Liu1994">Liu and Rubin 1994</a>; <a href="#ref-McLachlan1997">McLachlan and Krishnan 2007</a>)</span>, and is given in Figure <a href="#fig:ecmemean">1.1</a> below.</p>
<p>By using the idea that the additive variance model is considered to be a latent outcome model in which the observed outcome is the sum of independent outcomes, we allow each member of the covariate vector <span class="math inline">\(\boldsymbol{x}_i\)</span> to be additional latent variables to be estimated. Therefore, with a total of <span class="math inline">\(Q\)</span> variables in the variance model, there are <span class="math inline">\(Q+1\)</span> independent, unobserved, latent variables,
<span class="math display">\[\begin{align*}
Y_{i} \sim N(\boldsymbol{\beta} \boldsymbol{z}, \alpha_0 ) \textrm{,    } \quad
Z_{iq} \sim N(0, \alpha_q x_{iq}) ,
\end{align*}\]</span>
where <span class="math inline">\(X_i=Y_i+Z_{i1}+...+Z_{iQ}\)</span>.</p>
<p>In order to search the entire parameter space, a process of using sets of constrained EM algorithms are performed, an instance of a combinatorial EM (CEM) algorithm. Without loss of generality, assume that each covariate is scaled such that <span class="math inline">\(x_{i} \in [0,1]\)</span>. Therefore, the constant term in the variance model (<span class="math inline">\(\alpha_0\)</span>) will be the variance when all other variance parameters are zero. In order to search for non-positive slope, an EM algorithm is fit using the covariate <span class="math inline">\(1-x_i\)</span> in place of <span class="math inline">\(x_i\)</span>, and thus the EM algorithm is maximising the log-likelihood over the parameter space estimates for <span class="math inline">\(\alpha_0 \geq 0\)</span> and <span class="math inline">\(\alpha_q &lt; 0\)</span> for <span class="math inline">\(q=1,2,...,Q\)</span>. By repeating this for all possible covariate combinations, we have a total of <span class="math inline">\(2^Q\)</span> EM algorithms in order to search the entire parameter space. The MLE will then be the <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> from the EM algorithm that achieved the highest log-likelihood, where <span class="math inline">\(\boldsymbol{\theta}\)</span> is a vector of unknown parameters, <span class="math inline">\(\boldsymbol{\theta} = (\boldsymbol{\beta},\boldsymbol{\alpha})\)</span>. These <span class="math inline">\(2^Q\)</span> models are referred to as the family of complete data models, and the general algorithm is again an instance of a CEM algorithm. Standard errors are obtained from the Fisher information matrix <span class="citation">(<a href="#ref-Robledo2021">Robledo and Marschner 2021</a>)</span>, or by bootstrapping.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ecmemean"></span>
<img src="under_the_hood_files/figure-html/ecmemean-1.png" alt="The ECME algorithm for the estimation of the mean and variance" width="480"><p class="caption">
Figure 1.1: The ECME algorithm for the estimation of the mean and variance
</p>
</div>
</div>
<div class="section level2" number="2">
<h2 id="mean-and-variance-models-with-censored-outcome-data">
<span class="header-section-number">2</span> Mean and Variance models with censored outcome data<a class="anchor" aria-label="anchor" href="#mean-and-variance-models-with-censored-outcome-data"></a>
</h2>
<p>Censoring often occurs in the analysis of biomarker data, where the samples are measured with an assay that usually have detectable limits, with data at both upper and lower limits reached. When biomarker data can be below the detectable limit (left censored), and/or above the detectable limit (right censored), differences between two biomarker readings will also be censored. We need to be able to accurately model such data, particularly with a steady increase in the numbers of biomarkers investigated in clinical trials, and potentially large amounts of data above or below detectable limits. Some studies report over one-third of their biomarker data as below the detectable limit <span class="citation">(<a href="#ref-White2014">White et al. 2014</a>)</span>.</p>
<p>If <span class="math inline">\(X_i^*\)</span> is our true outcome data, let <span class="math inline">\(X_i\)</span> be the outcome data observed with censoring, and let us account for lower and upper limit censoring for completeness. This censored data given an additional level of missingness in our CEM algorithm, and thus an extra level of our complete data. If <span class="math inline">\(X^{(L)}\)</span> is the lower limit of detection and <span class="math inline">\(X^{(U)}\)</span> is the upper limit then
<span class="math display" id="eq:xi">\[\begin{align}
    X_i =
\begin{cases}
    X_i^*,          &amp; \text{if } X^{(L)} \leq X_i^* \leq X^{(U)}\\
    X^{(L)},        &amp; \text{if } X_i^* &lt; X^{(L)} \quad\quad \quad
    \textrm{for  } i = 1,2 .., n.\\
    X^{(U)},        &amp; \text{if } X_i^* &gt; X^{(U)}
\end{cases} \tag{2.1}
\end{align}\]</span></p>
<p>We will also need to specify <span class="math inline">\(c\)</span>, the censoring indicator, where</p>
<p><span class="math display">\[\begin{align*}
    c_i =
\begin{cases}
    0,          &amp; \text{if } X^{(L)} \leq X_i^* \leq X^{(U)}\\
    -1,        &amp; \text{if } X_i^* &lt; X^{(L)}\\
    1,      &amp; \text{if } X_i^* &gt; X^{(U)}.
\end{cases}
\end{align*}\]</span></p>
<p>Assuming still that the covariates <span class="math inline">\(x_{iq}\)</span> are scaled such that <span class="math inline">\(x_{iq} \in [0,1]\)</span>, let us now fit the same model as Equation <a href="#eq:general">(1.1)</a> however with <span class="math inline">\(X_i^*\)</span> as our outcome data.
If <span class="math inline">\(\Phi\)</span> is the standard normal cumulative distribution function and <span class="math inline">\(\phi\)</span> is the standard normal probability density function, then two useful functions are <span class="math inline">\(R(z)=\dfrac{\phi(z)}{1-\Phi(z)}\)</span> and <span class="math inline">\(Q(z)=\dfrac{-\phi(z)}{\Phi(z)}\)</span> <span class="citation">(<a href="#ref-Mills1926">Mills 1926</a>)</span>. Additionally, when <span class="math inline">\(z\)</span> tends to negative infinity, the approximation <span class="math inline">\(\dfrac{\phi(z)}{\Phi(z)} \approx -z\)</span> can be used.
The likelihood function for the observed data model with censored data is
<span class="math display">\[\begin{align*}
    l(\boldsymbol{\beta, \alpha}) = \prod_{i=1}^{N} l_i
\end{align*}\]</span>
where
<span class="math display">\[\begin{align*}
l_i =
\begin{cases}
    \displaystyle \frac{1}{\sigma(\boldsymbol{\alpha})} \phi \left( \frac{X_i-\mu(\boldsymbol{\beta}))}{\sigma(\boldsymbol{\alpha})} \right)  ,         &amp; \text{if } c_i=0\\
     \displaystyle 1- \Phi \left( \frac{\mu(\boldsymbol{\beta}) - X_i^{(L)}}{\sigma(\boldsymbol{\alpha})} \right)  ,        &amp; \text{if } c_i=-1\\
     \displaystyle \Phi \left( \frac{\mu(\boldsymbol{\beta}) - X_i^{(U)}}{\sigma(\boldsymbol{\alpha})} \right)  ,        &amp; \text{if } c_i=1.
\end{cases}
\end{align*}\]</span>
The corresponding log-likelihood is then
<span class="math display">\[\begin{align*}
    \ell(\boldsymbol{\beta, \alpha}) =  \sum_{i=1}^{N} \ell_i,
\end{align*}\]</span>
where
<span class="math display">\[\begin{align*}
\ell_i =
\begin{cases}
    \displaystyle \log \left( \frac{1}{\sigma(\boldsymbol{\alpha})} \phi \left( \frac{X_i- \mu(\boldsymbol{\beta})}{\sigma(\boldsymbol{\alpha})} \right)  \right),          &amp; \text{if } c_i=0\\
     \displaystyle \log \left( 1- \Phi \left( \frac{\mu(\boldsymbol{\beta}) - X_i^{(L)}}{\sigma(\boldsymbol{\alpha})} \right) \right) ,        &amp; \text{if } c_i=-1\\
     \displaystyle \log \left( \Phi \left( \frac{\mu(\boldsymbol{\beta}) - X_i^{(U)}}{\sigma(\boldsymbol{\alpha})} \right)  \right),        &amp; \text{if } c_i=1.
\end{cases}
\end{align*}\]</span>
Now in the CEM algorithm, the uncensored outcome variable <span class="math inline">\(X_i^*\)</span> will be assumed to be composed of <span class="math inline">\(Q+1\)</span> independent, unobserved, latent variables:
<span class="math display" id="eq:clatents">\[\begin{align}
X_i^* =Y_i+Z_{i1}+...+Z_{iQ} \quad \textrm{where}  \quad
Y_i &amp;\sim N(0, \alpha_0) \quad \textrm{and}  \nonumber \\
Z_{i1} &amp;\sim N(0,  \alpha_1x_{i1})  ,..., Z_{iQ} \sim N(0,  \alpha_Qx_{iQ}). \tag{2.2}
\end{align}\]</span>
This means that we will need to find the conditional expectations of <span class="math inline">\(Y_i^2\)</span> and <span class="math inline">\(Z_{i1}^2, ..., Z_{iQ}^2\)</span>, given the observed outcome value, <span class="math inline">\(X_i\)</span>, which may be censored.</p>
<p>From <span class="citation">(<a href="#ref-Aitkin1964">Aitkin 1964</a>)</span>, we see that for bivariate normal variables <span class="math inline">\(M\)</span> and <span class="math inline">\(N\)</span>, where
<span class="math display">\[\begin{align*}
\begin{pmatrix}M\\
N
\end{pmatrix} &amp;\sim  N
\begin{bmatrix}
\begin{pmatrix}
0\\
0
\end{pmatrix}\!\!,
\begin{pmatrix}
1 &amp; \rho \\
\rho &amp; 1
\end{pmatrix}
\end{bmatrix},
\end{align*}\]</span>
the conditional distribution for <span class="math inline">\(M\)</span> given censoring in <span class="math inline">\(N\)</span> can be obtained. In particular, given an upper limit, <span class="math inline">\(a\)</span>, it was shown that:
<span class="math display" id="eq:fcensor">\[\begin{align}
\mathbb{E}(M^2 \mid N&gt;a) = \frac{a\rho^2}{R(a)}+1, \tag{2.3}
\end{align}\]</span>
where <span class="math inline">\(\rho\)</span> is the correlation between <span class="math inline">\(M\)</span> and <span class="math inline">\(N\)</span> and <span class="math inline">\(R(a)\)</span> was defined previously. We know from Equation <a href="#eq:clatents">(2.2)</a> that the variance for <span class="math inline">\(Y_i\)</span> is <span class="math inline">\(\alpha_0\)</span>, and the variance for <span class="math inline">\(X_i\)</span> is <span class="math inline">\(\sigma^2(\boldsymbol{\alpha})\)</span>. So, given Equation <a href="#eq:clatents">(2.2)</a> and the general result in Equation <a href="#eq:fcensor">(2.3)</a>, it follows that,
<span class="math display" id="eq:fcens2">\[\begin{align}
\mathbb{E}(Y_i^2 \rvert X_i&gt;X^{(U)}) &amp;=\alpha_0 \mathbb{E}\left( \dfrac{Y_i^2}{{\alpha_0}} \rvert \dfrac{X_i}{\sigma(\boldsymbol{\alpha})}&gt;\dfrac{X^{(U)}}{\sigma(\boldsymbol{\alpha})}\right), \nonumber \\
&amp;=\alpha_0  \left( \dfrac{\dfrac{X^{(U)}}{\sigma(\boldsymbol{\alpha})} Corr(Y_i,X_i)^2 }{ R\left(\dfrac{X^{(U)}}{\sigma(\boldsymbol{\alpha})}\right)} +1 \right). \tag{2.4}
\end{align}\]</span>
We can then determine the correlation between <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_i\)</span>:
<span class="math display" id="eq:rho1">\[\begin{align}
Corr(Y_i, X_i) &amp;= \dfrac{cov(Y_i, X_i)}{\sqrt{ \alpha_0\sigma^2(\boldsymbol{\alpha})}} \nonumber \\
&amp;=  \dfrac{cov(Y_i,Y_i+Z_i)}{\sqrt{ \alpha_0\sigma^2(\boldsymbol{\alpha})}} \nonumber\\
&amp;= \dfrac{cov(Y_i, Y_i)+cov(Y_i,Z_i)}{\sqrt{ \alpha_0\sigma^2(\boldsymbol{\alpha})}} \nonumber\\
&amp;= \dfrac{\alpha_0+0}{\sqrt{ \alpha_0\sigma^2(\boldsymbol{\alpha})}}, \nonumber \\
&amp;= \dfrac{\sqrt{\alpha_0}}{\sigma(\boldsymbol{\alpha})}. \tag{2.5}
\end{align}\]</span>
We can also determine the correlation for each of the <span class="math inline">\(Z_{iq}\)</span> and <span class="math inline">\(X_i\)</span>, using a similar argument:
<span class="math display" id="eq:rho2">\[\begin{align}
Corr(Z_{iq}, X) &amp;= \dfrac{\sqrt{\alpha_qx_{iq}}}{\sigma(\boldsymbol{\alpha})}. \tag{2.6}
\end{align}\]</span>
Now we apply the correlation found in Equation <a href="#eq:rho1">(2.5)</a> to Equation <a href="#eq:fcens2">(2.4)</a>, in order to obtain our conditional expectation of <span class="math inline">\(Y_i^2\)</span>:
<span class="math display" id="eq:exp1">\[\begin{align}
\mathbb{E}(Y_i^2 \rvert X_i&gt;X^{(U)}) &amp;=\alpha_0  + \dfrac{\alpha_0^2 X^{(U)}}{\sigma^3(\boldsymbol{\alpha}) R\left(\dfrac{X^{(U)}}{\sigma_X}\right)}. \tag{2.7}
\end{align}\]</span>
The expectation of each of the <span class="math inline">\(Z_{iq}^2\)</span> follows the same argument, using Equation <a href="#eq:rho2">(2.6)</a>.</p>
<p>In order to apply these expectations to left censored data, we use the function <span class="math inline">\(Q(z)\)</span> rather than <span class="math inline">\(R(z)\)</span>. For censored outcome data, the E-step involves the calculation of the conditional expectations as defined above:</p>
<p><span class="math display" id="eq:calcyicens">\[\begin{align}
    \hat{Y}_i^2(\boldsymbol{\theta}) =
\begin{cases}
    \displaystyle \alpha_0+\dfrac{\alpha_0^2}{\sigma^2(\boldsymbol{\alpha})}\left( \dfrac{\left(\dfrac{X_i- \mu(\boldsymbol{\beta})}{\sigma(\boldsymbol{\alpha})}  \right)^2}{\sigma(\boldsymbol{\alpha})} -1\right),           &amp; \text{if } c_i=0 \\
     \displaystyle \alpha_0+\dfrac{\alpha_0^2}{\sigma^2(\boldsymbol{\alpha})}\left( \dfrac{\dfrac{ X_i- \mu(\boldsymbol{\beta})}{\sigma(\boldsymbol{\alpha})}}{Q\left(\dfrac{ X_i- \mu(\boldsymbol{\beta})}{\sigma(\boldsymbol{\alpha})}\right)}\right) ,        &amp; \text{if } c_i=-1 \\
     \displaystyle \alpha_0+\dfrac{\alpha_0^2}{\sigma^2(\boldsymbol{\alpha})}\left( \dfrac{\dfrac{ X_i- \mu(\boldsymbol{\beta})}{\sigma(\boldsymbol{\alpha})}}{R\left(\dfrac{ X_i- \mu(\boldsymbol{\beta})}{\sigma(\boldsymbol{\alpha})}\right)}\right),        &amp; \text{if } c_i=1,
\end{cases} \tag{2.8}
\end{align}\]</span></p>
<p>remembering that <span class="math inline">\(\boldsymbol{\theta}=(\boldsymbol{\beta},\boldsymbol{\alpha})\)</span>, and <span class="math inline">\(X_i\)</span> is defined in Equation <a href="#eq:xi">(2.1)</a>. The conditional expectations associated with the <span class="math inline">\(Z_{iq}\)</span> follow the same principles,</p>
<p><span class="math display" id="eq:calczicens">\[\begin{align}
\hat{Z}_{iq}^2(\boldsymbol{\theta}) =
\begin{cases}
    \displaystyle \alpha_q x_{iq}+\dfrac{\left( \alpha_q x_{iq} \right)^2}{\sigma^2(\boldsymbol{\alpha})}\left( \dfrac{\left( \dfrac{X_i- \mu(\boldsymbol{\beta})}{\sigma(\boldsymbol{\alpha})} \right)^2}{\sigma(\boldsymbol{\alpha})} -1 \right),         &amp; \text{if } c_i=0\\
     \displaystyle \alpha_q x_{iq}+\dfrac{\left( \alpha_q x_{iq} \right)^2}{\sigma^2(\boldsymbol{\alpha})}\left( \dfrac{\dfrac{ X_i- \mu(\boldsymbol{\beta})}{\sigma(\boldsymbol{\alpha})}}{Q\left( \dfrac{ X_i- \mu(\boldsymbol{\beta})}{\sigma(\boldsymbol{\alpha})}\right) } \right) ,        &amp; \text{if } c_i=-1\\
     \displaystyle \alpha_q x_{iq}+\dfrac{\left( \alpha_q x_{iq} \right)^2}{\sigma^2(\boldsymbol{\alpha})}\left( \dfrac{\dfrac{ X_i-\mu(\boldsymbol{\beta})}{\sigma(\boldsymbol{\alpha})}}{R\left(\dfrac{ X_i- \mu(\boldsymbol{\beta})}{\sigma(\boldsymbol{\alpha})}\right) }\right),        &amp; \text{if } c_i=1.
\end{cases} \tag{2.9}
\end{align}\]</span></p>
<p>The next step of the algorithm involves calculating the updated estimates of <span class="math inline">\(\boldsymbol{\theta}\)</span>, <span class="math inline">\(\boldsymbol{\hat{\theta}}^{new}\)</span>. The estimates for <span class="math inline">\(\hat{\boldsymbol{\alpha}}\)</span> for fixed <span class="math inline">\(\boldsymbol{\beta}\)</span> are obtained by the following,</p>
<p><span class="math display" id="eq:censupdatealpha">\[\begin{align}
\hat \alpha_0^{new}=n^{-1} \sum\limits_{i=1}^n \hat Y_i^2 \left( \hat{\boldsymbol{\theta}}^{old} \right) \quad
\textrm{and} \quad
\hat{\alpha}_q^{new}= \frac{\sum\limits_{i=1}^n  \hat{Z}_{iq}^2 (\boldsymbol{\theta}^{old})}{n \sum\limits_{i=1}^n  x_{iq}}. \tag{2.10}
\end{align}\]</span></p>
<p>Previously in the mean and variance model <span class="citation">(<a href="#ref-Robledo2021">Robledo and Marschner 2021</a>)</span>, a weighted linear regression was fit in order to obtain an updated estimate of the mean parameters (<span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) at each iteration, for fixed <span class="math inline">\(\boldsymbol{\alpha}=\boldsymbol{\hat{\alpha}}^{new}\)</span>. In order to estimate the mean model with censored outcome data, we need to fit a heteroscedastic censored linear regression model. This can be achieved by first standardizing the data, and then performing a homoscedastic censored linear regression at each iteration. In order to standardize the data, we divide by the standard deviation to obtain</p>
<p><span class="math display" id="eq:censoutcome">\[\begin{align}
\frac{X_i}{\sigma(\boldsymbol{\alpha})} \sim \textrm{censored } N\left( \dfrac{\mu(\boldsymbol{\beta})}{\sigma(\boldsymbol{\alpha})} ,1\right).  \tag{2.11}
\end{align}\]</span></p>
<p>A homoscedastic censored linear regression for <span class="math inline">\(\dfrac{X_i}{\sigma(\boldsymbol{\alpha})}\)</span> is then performed, against covariates <span class="math inline">\(\dfrac{z_{ip}}{\sigma(\boldsymbol{\alpha})}\)</span>, for fixed <span class="math inline">\(\boldsymbol{\alpha}=\boldsymbol{\hat{\alpha}}^{new}\)</span>. This can easily be implemented in standard software for censored normal linear regression, which can be performed with <code>survreg</code> in R. Once the censored regression is performed, the <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> estimates are back transformed by multiplying by the standard deviation <span class="math inline">\(\sigma(\boldsymbol{\alpha})\)</span> for our current fixed <span class="math inline">\(\boldsymbol{\alpha}\)</span>. This process is continued until convergence of the parameter estimates.</p>
<p>This algorithm is an instance of the ECME algorithm, and is summarised schematically in Figure xx. As detailed in Section xxx and xxx, the algorithm maximises the log-likelihood over a restricted parameter space, and will need to be run multiple times in order to maximise over the full parameter space. Thus a total of <span class="math inline">\(2^Q\)</span> ECME algorithms must be run, once for each combination of the <span class="math inline">\(q^{th}\)</span> variance covariate taking the value <span class="math inline">\(x_{i}\)</span> or <span class="math inline">\(1-x_{i}\)</span>, for <span class="math inline">\(q=1,2,...,Q\)</span>. The log-likelihood is then maximised over the entire parameter space with this family of ECME algorithms.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ecmecensored"></span>
<img src="under_the_hood_files/figure-html/ecmecensored-1.png" alt="The ECME algorithm for the estimation of the mean and variance with censored outcome data" width="480"><p class="caption">
Figure 2.1: The ECME algorithm for the estimation of the mean and variance with censored outcome data
</p>
</div>
<p>Standard errors are implemented by bootstrapping. In practice, this may take some time for datasets with many parameters fit in the variance model. This is due to the number of parameter spaces, and thus ECME algorithms, that must be run.</p>
</div>
<div class="section level2" number="3">
<h2 id="models-for-the-location-scale-and-shape">
<span class="header-section-number">3</span> Models for the location, scale and shape<a class="anchor" aria-label="anchor" href="#models-for-the-location-scale-and-shape"></a>
</h2>
<div class="section level3" number="3.1">
<h3 id="skew-normal-distribution">
<span class="header-section-number">3.1</span> Skew-normal distribution<a class="anchor" aria-label="anchor" href="#skew-normal-distribution"></a>
</h3>
<p>The skew-normal distribution is a distribution that extends the normal distribution to allow for non-zero skew <span class="citation">(<a href="#ref-Azzalini2013">Azzalini 2013</a>)</span>. This distribution has three parameters, the location parameter <span class="math inline">\(\xi\)</span> (<span class="math inline">\(\xi \in (-\infty, \infty)\)</span>), the scale parameter <span class="math inline">\(\omega\)</span> (<span class="math inline">\(\omega \in (0, \infty)\)</span>) and the shape parameter <span class="math inline">\(\nu\)</span> (<span class="math inline">\(\nu \in (-\infty, \infty)\)</span>). If <span class="math inline">\(\nu&lt;0\)</span>, the distribution is left skewed, and if <span class="math inline">\(\nu &gt;0\)</span> then the distribution is right skewed. The normal distribution is recovered with <span class="math inline">\(\nu=0\)</span>.</p>
<p>The probability density function of the skew normal is</p>
<p><span class="math display">\[\begin{align*}
f(x)= \frac{2}{\omega}\phi \left( \frac{x-\xi}{\omega} \right)\Phi \left(\nu \left(\frac{x-\xi}{\omega}\right) \right)\quad -\infty &lt; x &lt; \infty
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\Phi\)</span> are the density and distribution functions of the standard normal distribution, respectively.
If a random variable <span class="math inline">\(X\)</span> has a skew-normal distribution with parameters <span class="math inline">\((\xi, \omega, \nu)\)</span>, this is written as</p>
<p><span class="math display">\[\begin{align*}
X \sim  SN\left(\xi, \omega^2, \nu\right).
\end{align*}\]</span></p>
</div>
<div class="section level3" number="3.2">
<h3 id="maximum-likelihood-estimation">
<span class="header-section-number">3.2</span> Maximum likelihood estimation<a class="anchor" aria-label="anchor" href="#maximum-likelihood-estimation"></a>
</h3>
<p>If <span class="math inline">\(X_1, ..., X_n\)</span> are independent and identically distributed observations from <span class="math inline">\(SN(\xi, \omega^2, \nu)\)</span>, the likelihood function for the sample is given as
<span class="math display" id="eq:llsn">\[\begin{align}
\mathcal{L}(\xi, \omega, \nu)=\frac{2^n}{\omega^n} \prod\limits_{i=1}^n \phi \left(\frac{X_i-\xi}{\omega} \right) \Phi \left( \nu \left( \frac{X_i-\xi}{\omega}\right) \right), \tag{3.1}
\end{align}\]</span>
and the corresponding log-likelihood (omitting the constant term) reduces to
<span class="math display">\[\begin{align}
L(\xi, \omega, \nu)=- n\log(\omega) + \sum\limits_{i=1}^n \log \phi \left(\frac{X_i-\xi}{\omega}\right) + \sum\limits_{i=1}^n  \log \Phi \left(\nu \left(\frac{X_i-\xi}{\omega}\right) \right).
\end{align}\]</span></p>
<p>As will be explained below, the likelihood in Equation <a href="#eq:llsn">(3.1)</a> is a censored Gaussian likelihood when viewed as a function of <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\omega\)</span>, and a probit regression likelihood when viewed as a function of <span class="math inline">\(\nu\)</span>. This is useful as it allows a straightforward cyclic coordinate ascent algorithm to be used to obtain the MLE. For optimisation of a multi-variable function, a cyclic coordinate ascent algorithm involves the optimisation of a function with respect to one variable holding the other variables constant, and then repeating with respect to each of the variables <span class="citation">(<a href="#ref-Lange2013">Lange 2013</a>)</span>. In our context, the cyclic coordinate ascent algorithm will cycle between a censored Gaussian model (keeping <span class="math inline">\(\nu\)</span> constant), and a probit regression (keeping <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\omega\)</span> constant).</p>
<p>Although there already exists an algorithm for obtaining the MLE in the skew-normal model in the <code>SN</code> package <span class="citation">(<a href="#ref-Azzpackage">Azzalini 2016</a>)</span> in R, we introduce this new method as it more easily generalises to regression modelling.</p>
<p>In order to see that <a href="#eq:llsn">(3.1)</a> is equivalent to a probit regression likelihood when viewed as a function of <span class="math inline">\(\nu\)</span> alone, we note that <a href="#eq:llsn">(3.1)</a> is proportional to the following function of <span class="math inline">\(\nu\)</span>,
<span class="math display" id="eq:l2">\[\begin{align}
\mathcal{L}_2(\nu|\xi, \omega) &amp;=  \prod_{i=1}^n \Phi \left( \nu \left( \frac{X_i-\xi}{\omega}\right) \right).  \tag{3.2}
\end{align}\]</span>
Now let the residuals be defined as follows, for fixed <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\omega\)</span>,
<span class="math display">\[\begin{align*}
r_i = \frac{X_i-\xi}{\omega}.
\end{align*}\]</span></p>
<p>Then, <a href="#eq:l2">(3.2)</a> is
<span class="math display">\[\begin{align*}
\mathcal{L}_2(\nu|\xi, \omega) &amp;=  \prod_{i=1}^n \Phi \left( \nu r_i \right) =
\prod_{r_i \geq 0} \Phi(\nu |r_i|) \prod_{r_i &lt; 0} \left[ 1-\Phi(\nu |r_i|) \right].
\end{align*}\]</span>
This is exactly a probit regression likelihood with the binary outcome
<span class="math display">\[\begin{align*}
    V_i =
\begin{cases}
    \displaystyle 1         &amp; \text{if } r_i \geq 0\\
     \displaystyle 0        &amp; \text{if } r_i&lt;0
\end{cases}
\end{align*}\]</span>
and the linear predictor being <span class="math inline">\(\nu |r_i|\)</span>. Thus, for fixed <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\omega\)</span>, <a href="#eq:llsn">(3.1)</a> can be maximised as a function of <span class="math inline">\(\nu\)</span> by fitting a probit regression on the binary outcome <span class="math inline">\(V_i\)</span>, with no intercept and a single covariate <span class="math inline">\(|r_i|\)</span>.</p>
<p>Next, let us look at the estimation of the <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\omega\)</span> parameters, holding <span class="math inline">\(\nu\)</span> constant. If we examine <a href="#eq:llsn">(3.1)</a>, we will now show how this can be considered to be a censored Gaussian likelihood, where the first component is with regards to the uncensored data, and the second component is with regards to the censored data (and multiplied by our constant <span class="math inline">\(\nu\)</span>). Note as well, that each observation contributes to both the censored and uncensored components of the likelihood.</p>
<p>For the purpose of performing a censored regression, we will utilise the algorithm developed above for censored outcome data. In order to make use of this algorithm, the data needs to be manipulated to ensure that each observation is contributing to both components of the likelihood. Firstly, we create a censoring indicator, <span class="math inline">\(c\)</span>, of length <span class="math inline">\(2n\)</span>. The first <span class="math inline">\(n\)</span> elements are set to <span class="math inline">\(0\)</span>, and then elements <span class="math inline">\(n+1, ..., 2n\)</span> are set to <span class="math inline">\(-1\)</span> to indicate left censoring. Next, we create a vector of <span class="math inline">\(1\)</span>s called <span class="math inline">\(I\)</span> of length <span class="math inline">\(2n\)</span>, and we also create a new outcome vector called <span class="math inline">\(D\)</span>, where observations <span class="math inline">\(1, ..., n\)</span> are <span class="math inline">\(X\)</span>, and observations <span class="math inline">\(n+1, ..., 2n\)</span> are also <span class="math inline">\(X\)</span>. This corresponds to
<span class="math display">\[ D=
\left( \begin{array}{c}
    X_1      \\
    \vdots \\
    X_n \\
    X_1 \\
    \vdots \\
    X_n     
\end{array} \right), ~~~
I=
\left( \begin{array}{c}
    1      \\
    \vdots \\
    1    \\
    1 \\
    \vdots \\
    1    
\end{array} \right), ~~ \text{and} ~~
c=
\left( \begin{array}{c}
    0      \\
    \vdots \\
    0    \\
    1 \\
    \vdots \\
    1    
\end{array} \right).
\]</span>
Now, the likelihood for the observed data model is
<span class="math display">\[\begin{align*}
l(\xi, \omega | \nu) = \prod_{i=1}^{2n} l_i ,
\end{align*}\]</span>
where
<span class="math display">\[\begin{align*}
l_i=
\begin{cases}
    \displaystyle  \frac{1}{\omega} \phi \left( \frac{D_i-I_i}{\omega} \right)  ,           &amp; \text{if } c_i=0 \\
     \displaystyle 1- \Phi \left( \nu \left( \frac{I_i - D_i}{\omega}\right)  \right)  ,        &amp; \text{if } c_i=-1,
\end{cases}
\end{align*}\]</span>
which equates to
<span class="math display" id="eq:ll2">\[\begin{align}
l_i=
\begin{cases}
    \displaystyle  \frac{1}{\omega} \phi \left( \frac{D_i-I_i}{\omega} \right)  ,           &amp; \text{if } c_i=0 \\
     \displaystyle  \Phi \left( \nu \left( \frac{D_i-I_i}{\omega} \right) \right)  ,        &amp; \text{if } c_i=-1.
\end{cases} \tag{3.3}
\end{align}\]</span>
Now we can compare Equations <a href="#eq:llsn">(3.1)</a> and <a href="#eq:ll2">(3.3)</a>, and see that they follow a similar form, where <span class="math inline">\(D_i = X_i\)</span> and <span class="math inline">\(I_i = \xi\)</span>.</p>
<p>So, we can fit our censored regression model to update <span class="math inline">\(\hat{\xi}\)</span> and <span class="math inline">\(\hat{\omega}\)</span> with our new outcome data <span class="math inline">\(D_i\)</span>, using the censoring indicator (<span class="math inline">\(c_i\)</span>) to indicate the censored observations. Note that the censored data observations require <span class="math inline">\(I_i\)</span> and <span class="math inline">\(D_i\)</span> to be multiplied by the constant <span class="math inline">\(\nu\)</span>. The location model contains only the vector <span class="math inline">\(I_i\)</span> as a covariate, that is, with no intercept term. Meanwhile, the scale model contains an intercept only. The parameter from the <span class="math inline">\(I_i\)</span> covariate is the estimate for <span class="math inline">\(\hat{\xi}\)</span> and the intercept estimate from the scale model is <span class="math inline">\(\hat{\omega}\)</span>.</p>
<p>Once we have calculated our current estimates for <span class="math inline">\(\hat{\xi}\)</span>, <span class="math inline">\(\hat{\omega}\)</span> and <span class="math inline">\(\hat{\nu}\)</span>, we then examine convergence previously <span class="citation">(<a href="#ref-Robledo2021">Robledo and Marschner 2021</a>)</span>. If convergence has not been met, we iterate our cyclic coordinate ascent algorithm by re-calculating the residuals, performing another probit regression and then performing our censored Gaussian regression, until we do reach our convergence criteria. It is also of importance to note that whether we use the censored normal algorithm above, or another censored normal algorithm such as that provided in the <code>SN</code> package, we obtain the same result.</p>
</div>
<div class="section level3" number="3.3">
<h3 id="extension-to-lss-regression-model">
<span class="header-section-number">3.3</span> Extension to LSS regression model<a class="anchor" aria-label="anchor" href="#extension-to-lss-regression-model"></a>
</h3>
<p>Now that an algorithm has been developed to estimate the <span class="math inline">\(\xi\)</span>, <span class="math inline">\(\omega\)</span> and <span class="math inline">\(\nu\)</span> parameters in a skew normal model, let us extend this to incorporate a regression model for each of the three parameters. In this case the model becomes
<span class="math display">\[\begin{align*}
X_i \sim SN\left(\xi_0+\sum\limits_{p=1}^P \xi_p s_{ip},~   \omega_0+\sum\limits_{q=1}^Q \omega_q l_{iq}, ~  \nu_0+\sum\limits_{k=1}^K \nu_k u_{ik} \right) \quad \textrm{for } i=1, 2,\dots,n,
\end{align*}\]</span>
where we have covariates <span class="math inline">\(s_{ip}\)</span> (<span class="math inline">\(p=1,...,P\)</span>) for the location model, covariates <span class="math inline">\(l_{iq}\)</span> (<span class="math inline">\(q=1,...,Q\)</span>) for the scale model and covariates <span class="math inline">\(u_{ik}\)</span> (<span class="math inline">\(k=1,...,K\)</span>) for the shape model.</p>
<p>The likelihood for this model is
<span class="math display" id="eq:llext">\[\begin{align}
\mathcal{L}(\boldsymbol{\xi},\boldsymbol{\omega}, \boldsymbol{\nu}) = \left(\frac{2}{\left(\sqrt{\omega_0 + \sum\limits_{q=1}^Q \omega_q l_{iq}}\right)}\right)^n \prod\limits_{i=1}^n \phi \left(\frac{ X_i-\xi_0 - \sum\limits_{p=1}^P \xi_p s_{ip} }{\sqrt{\omega_0+\sum\limits_{q=1}^Q \omega_q l_{iq}}} \right) \nonumber


\Phi \left( \left( \nu_0+\sum\limits_{k=1}^K \nu_k u_{ik} \right) \left( \frac{( X_i-\xi_0 - \sum\limits_{p=1}^P \xi_p s_{ip}}{\sqrt{\omega_0+\sum\limits_{q=1}^Q \omega_q l_{iq}}}\right) \right) ,  \tag{3.4}
\end{align}\]</span>
and the log-likelihood reduces to
<span class="math display">\[\begin{align*}
L(\boldsymbol{\xi},\boldsymbol{\omega}, \boldsymbol{\nu})=- n \log \left(\sqrt{\omega_0 + \sum\limits_{q=1}^Q \omega_q l_{iq}} \right) + \sum\limits_{i=1}^n \log \frac{\left( X_i-\xi_0 - \sum\limits_{p=1}^P \xi_p s_{ip} \right)}{\sqrt{\omega_0+\sum\limits_{q=1}^Q \omega_q l_{iq}}} \\
+ \sum\limits_{i=1}^n  \log \Phi \left(\left( \nu_0+\sum\limits_{k=1}^K \nu_k u_{ik}\right) \frac{X_i-\xi_0 - \sum\limits_{p=1}^P \xi_p s_{ip} }{\sqrt{\omega_0+\sum\limits_{q=1}^Q \omega_q l_{iq}}} \right), \\
\end{align*}\]</span>
where <span class="math inline">\(\boldsymbol{\xi} = (\xi_1, \xi_2, ..., \xi_P)\)</span>, <span class="math inline">\(\boldsymbol{\omega}= (\omega_1, \omega_2, ..., \omega_Q)\)</span> and <span class="math inline">\(\boldsymbol{\nu}=(\nu_1, \nu_2, ..., \nu_K)\)</span>.</p>
<p>Similarly to the approach taken in the previous section, the likelihood in <a href="#eq:llext">(3.4)</a> can be seen as a censored Gaussian likelihood when viewed as a function of <span class="math inline">\(\boldsymbol{\xi}\)</span> and <span class="math inline">\(\boldsymbol{\omega}\)</span>, and a probit regression likelihood when viewed as a function of <span class="math inline">\(\boldsymbol{\nu}\)</span>. Therefore, we can take the same approach as before, with an extension of the cyclic coordinate ascent algorithm.</p>
<p>The steps outlined in the section above may be generalised as follows. First, the residuals can be defined as follows, for fixed <span class="math inline">\(\boldsymbol{\xi}\)</span> and <span class="math inline">\(\boldsymbol{\omega}\)</span>,
<span class="math display">\[\begin{align*}
r_i = \frac{X_i-\xi_0- \sum\limits_{p=1}^P \xi_p s_{ip}}{\sqrt{{\omega}_0 + \sum\limits_{q=1}^Q \omega_q l_{iq}}}.
\end{align*}\]</span>
Now,
<span class="math display">\[\begin{align*}
l_2(\boldsymbol{\nu}|\boldsymbol{\xi}, \boldsymbol{\omega}) &amp;=  \prod_{i=1}^n \Phi \left( \left( \nu_0+\sum\limits_{k=1}^K \nu_k u_{ik} \right) r_i \right) \\
&amp;=
\prod_{r_i \geq 0} \Phi \left( \left(\nu_0+\sum\limits_{k=1}^K \nu_k u_{ik}\right) |r_i| \right) \prod_{r_i &lt; 0} \left[ 1-\Phi \left( \left(\nu_0+\sum\limits_{k=1}^K \nu_k u_{ik}\right) |r_i| \right) \right].
\end{align*}\]</span>
As in section 3.2 above, this is exactly a probit regression likelihood with the binary outcome (<span class="math inline">\(V_i\)</span>). Therefore for fixed <span class="math inline">\(\boldsymbol{\xi}\)</span> and <span class="math inline">\(\boldsymbol{\omega}\)</span>, the likelihood <a href="#eq:llext">(3.4)</a> can be maximised as a function of <span class="math inline">\(\boldsymbol{\nu}\)</span> by fitting a probit regression on the binary outcome <span class="math inline">\(V_i\)</span> with the covariate <span class="math inline">\(|r_i|\)</span>, and each covariate of interest <span class="math inline">\(u_{ik}\)</span> multiplied by <span class="math inline">\(|r_i|\)</span>, with no intercept term. The estimate of <span class="math inline">\(\nu_0\)</span> is obtained from the estimate from the <span class="math inline">\(\lvert r_i \rvert\)</span> parameter in the model, and each of the estimates of <span class="math inline">\(\nu_{k}\)</span> are obtained from the parameter for the relevant <span class="math inline">\(u_{ik}\)</span> covariate in the model.</p>
<p>Next, let us consider the estimation of <span class="math inline">\(\boldsymbol{\xi}\)</span> and <span class="math inline">\(\boldsymbol{\omega}\)</span>, while maintaining <span class="math inline">\(\boldsymbol{\nu}\)</span> constant. Firstly, similar to Section 3.2, we must create our censoring indicator <span class="math inline">\(c\)</span> and our new outcome vector <span class="math inline">\(D\)</span>. Also, we must create a new variable for each of the covariates <span class="math inline">\(s_{ip}\)</span> which is of length <span class="math inline">\(2n\)</span>. For each of these covariates, the values <span class="math inline">\(i=1,...,n\)</span> are the values from <span class="math inline">\(s_{ip}\)</span> <span class="math inline">\(i=1,...,n\)</span>, and the values <span class="math inline">\(i=n+1, ..., 2n\)</span> are also <span class="math inline">\(s_{ip}\)</span>. Let us call these new covariates of length <span class="math inline">\(2n\)</span>, <span class="math inline">\(z_{ip}\)</span>.</p>
<p>In the same manner, the scale covariates are each manipulated to be of length <span class="math inline">\(2n\)</span>. Each covariate, <span class="math inline">\(l_{iq}\)</span>, is duplicated so that the first set of values, <span class="math inline">\(i=1,..n\)</span>, are the respective value from <span class="math inline">\(l_{iq}\)</span>. Then the next values, <span class="math inline">\(i=n+1,..,2n\)</span>, are also the respective value from <span class="math inline">\(l_{iq}\)</span>. Let us call these new covariates <span class="math inline">\(x_{iq}\)</span>.</p>
<p>Section 2 describes the extension for the censored regression model for <span class="math inline">\(\boldsymbol{\mu}\)</span> and <span class="math inline">\(\boldsymbol{\sigma^2}\)</span>, so our observed data likelihood for this extended model is
<span class="math display">\[\begin{align*}
l(\boldsymbol{\xi}, \boldsymbol{\omega} | \boldsymbol{\nu}) = \prod_{i=1}^{2n} l_i ,
\end{align*}\]</span>
where
<span class="math display">\[\begin{align*}
l_i=
\begin{cases}
    \displaystyle  \frac{1}{\sqrt{\omega_0+\sum\limits_{q=1}^Q \omega_q x_{iq}}} \phi \left( \frac{D_i-\xi_0 - \sum\limits_{p=1}^P \xi_p z_{ip}}{\sqrt{\omega_0+\sum\limits_{q=1}^Q \omega_q x_{iq}}} \right)  ,            &amp; \text{if } c_i=0 \\
     \displaystyle 1- \Phi \left( \left(\nu_0+\sum\limits_{k=1}^K \nu_k u_{ik}\right) \left( \frac{\xi_0 - \sum\limits_{p=1}^P \xi_p z_{ip}- D_i}{\sqrt{\omega_0+\sum\limits_{q=1}^Q \omega_q x_{iq}}}\right)  \right)  ,        &amp; \text{if } c_i=-1,
\end{cases}
\end{align*}\]</span>
which equates to
<span class="math display" id="eq:ll2ext">\[\begin{align}
l_i=
\begin{cases}
    \displaystyle  \frac{1}{\sqrt{\omega_0+\sum\limits_{q=1}^Q \omega_q x_{iq}}} \phi \left( \frac{D_i-\xi_0 - \sum\limits_{p=1}^P \xi_p z_{ip}}{\sqrt{\omega_0+\sum\limits_{q=1}^Q \omega_q x_{iq}}} \right)  ,            &amp; \text{if } c_i=0 \\
     \displaystyle  \Phi \left( \left(\nu_0+\sum\limits_{k=1}^K \nu_k u_{ik}\right) \left( \frac{D_i-\xi_0 - \sum\limits_{p=1}^P \xi_p z_{ip}}{\sqrt{\omega_0+\sum\limits_{q=1}^Q \omega_q x_{iq}}}\right)  \right)  ,        &amp; \text{if } c_i=-1.
\end{cases}  \tag{3.5}
\end{align}\]</span></p>
<p>Once the current estimates for <span class="math inline">\(\boldsymbol{\xi}\)</span>, <span class="math inline">\(\boldsymbol{\omega^2}\)</span> and <span class="math inline">\(\boldsymbol{\nu}\)</span> are obtained, the cyclic coordinate ascent algorithm continues to cycle between a probit regression and a censored normal regression until convergence. The algorithm is summarised schematically in Figure <a href="#fig:cyclicascent">3.1</a>. In this algorithm, estimation of standard errors is obtained by bootstrapping only.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:cyclicascent"></span>
<img src="under_the_hood_files/figure-html/cyclicascent-1.png" alt="The cyclic coordinate ascent algorithm for the estimation of the location, scale and shape" width="480"><p class="caption">
Figure 3.1: The cyclic coordinate ascent algorithm for the estimation of the location, scale and shape
</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Aitkin1964" class="csl-entry">
Aitkin, M. A. 1964. <span>“Correlation in a Singly Truncated Bivariate Normal Distribution.”</span> <em>Psychometrika</em> 29 (3): 263–70. <a href="https://doi.org/10.1007/BF02289723" class="external-link">https://doi.org/10.1007/BF02289723</a>.
</div>
<div id="ref-Azzalini2013" class="csl-entry">
Azzalini, Adelchi. 2013. <em>The Skew-Normal and Related Families</em>. Cambridge University Press.
</div>
<div id="ref-Azzpackage" class="csl-entry">
———. 2016. <em>The <span>R</span> Package <code>sn</code>: The Skew-Normal and Skew-<span class="math inline">\(t\)</span> Distributions (Version 1.4-0)</em>. Università di Padova, Italia. <a href="http://azzalini.stat.unipd.it/SN" class="external-link">http://azzalini.stat.unipd.it/SN</a>.
</div>
<div id="ref-Lange2013" class="csl-entry">
Lange, Kenneth. 2013. <em>Optimization</em>. Springer.
</div>
<div id="ref-Liu1994" class="csl-entry">
Liu, Chuanhai, and Donald B. Rubin. 1994. <span>“The ECME Algorithm: A Simple Extension of EM and ECM with Faster Monotone Convergence.”</span> <em>Biometrika</em> 81 (4): 633–48. <a href="https://doi.org/10.2307/2337067" class="external-link">https://doi.org/10.2307/2337067</a>.
</div>
<div id="ref-McLachlan1997" class="csl-entry">
McLachlan, Geoffrey J., and Thriyambakam Krishnan. 2007. <em>The EM Algorithm and Extensions</em>. New York: Wiley. <a href="https://doi.org/10.1002/9780470191613" class="external-link">https://doi.org/10.1002/9780470191613</a>.
</div>
<div id="ref-Mills1926" class="csl-entry">
Mills, John P. 1926. <span>“Table of the Ratio: Area to Bounding Ordinate, for Any Portion of Normal Curve.”</span> <em>Biometrika</em> 18 (3/4): 395–400. <a href="https://doi.org/10.2307/2331957" class="external-link">https://doi.org/10.2307/2331957</a>.
</div>
<div id="ref-Robledo2021" class="csl-entry">
Robledo, K. P., and I. C. Marschner. 2021. <span>“A New Algorithm for Fitting Semi-Parametric Variance Regression Models.”</span> <em>Computational Statistics</em>. <a href="https://doi.org/10.1007/s00180-021-01067-6" class="external-link">https://doi.org/10.1007/s00180-021-01067-6</a>.
</div>
<div id="ref-White2014" class="csl-entry">
White, Harvey D., Andrew Tonkin, John Simes, Ralph Stewart, Kristy Mann, Peter Thompson, David Colquhoun, et al. 2014. <span>“Association of Contemporary Sensitive Troponin i Levels at Baseline and Change at 1 Year with Long-Term Coronary Events Following Myocardial Infarction or Unstable Angina. Results from the LIPID Study (Long-Term Intervention with Pravastatin in Ischaemic Disease).”</span> <em>Journal of the American College of Cardiology</em> 63 (4): 345–54. <a href="https://doi.org/10.1016/j.jacc.2013.08.1643" class="external-link">https://doi.org/10.1016/j.jacc.2013.08.1643</a>.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Kristy Robledo.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
